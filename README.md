# Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation

README file building in progress.
If you'd like to have a quick try, simply go for the following command and you will get the LLMs output with MIRAGE AA results marked as citations:
```
python mirage.py --f data_input/example.json --config configs/llama2_selfcitation_prompt.yaml
```

This repository provides an easy-to-use MIRAGE framework for analyzing the groundedness of RAG generation to the retrieved documents. To re-implement the results in the paper, please refer to [this repo](github).
